import argparse
import torch.nn as nn
import numpy as np
import os

from functools import partial

from mvit.utils.misc import validate_checkpoint_wrapper_import
from mvit.models.attention import MultiScaleBlock
from mvit.models.common import round_width
from mvit.config.defaults import assert_and_infer_cfg, get_cfg
from mvit.utils.env import checkpoint_pathmgr as pathmgr

import logging
logger = logging.getLogger(__name__)

try:
    from fairscale.nn.checkpoint import checkpoint_wrapper
except ImportError:
    checkpoint_wrapper = None

import math
import torch
from torch.nn.init import trunc_normal_
import torchvision.transforms.functional as F
from PIL import Image
from torchvision import transforms

import configparser
from pathlib import Path
import shutil

### define mvit model class and other supplementary classes and funcs>>>>>>>>>>>>>>>

class MyConfigParser(configparser.RawConfigParser):
    def get(self, section, option):
        val = configparser.RawConfigParser.get(self, section, option)
        return val.strip('"').strip("'")

config_obj = MyConfigParser()
# check if config.ini is available, else make a copy of config.ini.bak and rename it to config.ini
config_path = "./config.ini"
if not Path(config_path).is_file():
    shutil.copyfile("config.ini.bak","config.ini")
config_obj.read(config_path)


cfg = get_cfg()
## if you haven't trained before, the initial state is just the pretrained model weights
cfg.OUTPUT_DIR = config_obj.get("MODEL","OUTPUT_DIR") # can change this if you like 
model_name = config_obj.get("MODEL","MODEL_NAME") # can change this if you like if you change models
my_checkpoint_path = cfg.OUTPUT_DIR + '/' + model_name # don't change this 

cfg.MVIT.POOL_KV_STRIDE =[]
# T model settings
cfg.MVIT.DROPPATH_RATE= 0.1
cfg.MVIT.DEPTH= 10
cfg.MVIT.DIM_MUL= [[1, 2.0], [3, 2.0], [8, 2.0]]
cfg.MVIT.HEAD_MUL= [[1, 2.0], [3, 2.0], [8, 2.0]]
cfg.MVIT.POOL_KVQ_KERNEL= [3, 3]
cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE= [4, 4]
cfg.MVIT.POOL_Q_STRIDE= [[0, 1, 1], [1, 2, 2], [2, 1, 1], [3, 2, 2], [4, 1, 1], [5, 1, 1], [6, 1, 1], [7, 1, 1], [8, 2, 2], [9, 1, 1]]
# B model settings
# cfg.MVIT.DROPPATH_RATE= 0.3
# cfg.MVIT.DEPTH= 24
# cfg.MVIT.DIM_MUL= [[2, 2.0], [5, 2.0], [21, 2.0]]
# cfg.MVIT.HEAD_MUL= [[2, 2.0], [5, 2.0], [21, 2.0]]
# cfg.MVIT.POOL_KVQ_KERNEL= [3, 3]
# cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE= [4, 4]
# cfg.MVIT.POOL_Q_STRIDE= [[0, 1, 1], [1, 1, 1], [2, 2, 2], [3, 1, 1], [4, 1, 1], [5, 2, 2], [6, 1, 1], [7, 1, 1], [8, 1, 1], [9, 1, 1], [10, 1, 1], [11, 1, 1], [12, 1, 1], [13, 1, 1], [14, 1, 1], [15, 1, 1], [16, 1, 1], [17, 1, 1], [18, 1, 1], [19, 1, 1], [20, 1, 1], [21, 2, 2], [22, 1, 1], [23, 1, 1]]

# Solver settings are the same for T and B models
cfg.SOLVER.BASE_LR_SCALE_NUM_SHARDS= True
cfg.SOLVER.BASE_LR= 0.00025
cfg.SOLVER.LR_POLICY= 'cosine'
cfg.SOLVER.WEIGHT_DECAY= 0.05
cfg.SOLVER.OPTIMIZING_METHOD= 'adamw'
cfg.SOLVER.CLIP_GRAD_L2NORM= 1.0

#### m1 mac settings, mainly for LF - disabling
# try:
#     torch.has_mps
#     if torch.has_mps:
#         use_mac = 1
#     else:
#         use_mac=0
# except:
#     use_mac = 0
use_mac=0

##### GENERALLY SPEAKING BELOW ARE THE MAIN STUFF TO CONFIGURE
if torch.cuda.is_available(): 
    cfg['NUM_GPUS'] = 1
else:
    cfg['NUM_GPUS'] = 0
if use_mac == 1:
    device = torch.device("mps")    
if torch.cuda.is_available():
    cfg.DATA_LOADER.NUM_WORKERS = 2 # we're setting this to 0 so that it doesn't use multi-processing, but i'm not sure if this is problematic for GPU...
else:
    cfg.DATA_LOADER.NUM_WORKERS = 0
cfg.MODEL.NUM_CLASSES = int(config_obj.get("MODEL","NUM_CLASSES"))
cfg.DATA.PATH_TO_DATA_DIR = config_obj.get("DATA","PATH_TO_DATA_DIR")
cfg.TRAIN.BATCH_SIZE = int(config_obj.get("TRAIN","BATCH_SIZE"))
cfg.TEST.BATCH_SIZE = int(config_obj.get("TEST","BATCH_SIZE"))
# set epoch to train to
cfg.SOLVER.MAX_EPOCH = int(config_obj.get("SOLVER","MAX_EPOCH"))
cfg.SOLVER.WARMUP_EPOCHS = int(config_obj.get("WARMUP","WARMUP_EPOCHS")) # set number of warmups epochs - defaults to 20 
# seems like certain epochs are evaluation epochs?
cfg.TRAIN.AUTO_RESUME = False
cfg.MIXUP.ENABLE = False
my_layers_frozen = config_obj.get("FREEZE","BLOCKS_FROZEN") # can change this if you like 
# adding data augmentation configurations
if config_obj.get("DATA_AUG","COLOR_JITTER") == "None":
    cfg.AUG.COLOR_JITTER = None
else:
    cfg.AUG.COLOR_JITTER = float(config_obj.get("DATA_AUG","COLOR_JITTER"))

if config_obj.get("DATA_AUG","RAND_AUG") == 'None':
    cfg.AUG.AA_TYPE = None
else:
    cfg.AUG.AA_TYPE = config_obj.get("DATA_AUG","RAND_AUG")
cfg.AUG.RE_PROB = float(config_obj.get("DATA_AUG","ERASE"))
cfg.AUG.NUM_SAMPLE = int(config_obj.get("DATA_AUG","NUM_COUNT"))
if config_obj.get("DATA_AUG","MIXUP") == 'False':
    cfg.MIXUP.ENABLE = False # default is false
else:
    cfg.MIXUP.ENABLE = True

def load_checkpoint(
    path_to_checkpoint,
    model,
    data_parallel=False,
    optimizer=None,
    scaler=None,
    epoch_reset=False,
    squeeze_temporal=False,
):
    """
    Load the checkpoint from the given file.
    Args:
        path_to_checkpoint (string): path to the checkpoint to load.
        model (model): model to load the weights from the checkpoint.
        data_parallel (bool): if true, model is wrapped by
        torch.nn.parallel.DistributedDataParallel.
        optimizer (optim): optimizer to load the historical state.
        scaler (GradScaler): GradScaler to load the mixed precision scale.
        epoch_reset (bool): if True, reset #train iterations from the checkpoint.
        squeeze_temporal (bool): if True, squeeze temporal dimension for 3D conv to
            2D conv.
    Returns:
        (int): the number of training epoch of the checkpoint.
    """
    assert pathmgr.exists(path_to_checkpoint), "Checkpoint '{}' not found".format(
        path_to_checkpoint
    )

    logger.info("Loading network weights from {}.".format(path_to_checkpoint))

    # Account for the DDP wrapper in the multi-gpu setting.
    ms = model.module if data_parallel else model

    # Load the checkpoint on CPU to avoid GPU mem spike.
    with pathmgr.open(path_to_checkpoint, "rb") as f:
        checkpoint = torch.load(f, map_location="cpu")

    pre_train_dict = checkpoint["model_state"]
    model_dict = ms.state_dict()

    if squeeze_temporal:
        for k, v in pre_train_dict.items():
            # convert 3D conv to 2D
            if (
                k in model_dict
                and len(v.size()) == 5
                and len(model_dict[k].size()) == 4
                and v.size()[2] == 1
            ):
                pre_train_dict[k] = v.squeeze(2)

    # Match pre-trained weights that have same shape as current model.
    pre_train_dict_match = {
        k: v
        for k, v in pre_train_dict.items()
        if k in model_dict and v.size() == model_dict[k].size()
    }
    # Weights that do not have match from the pre-trained model.
    not_load_layers = [
        k for k in model_dict.keys() if k not in pre_train_dict_match.keys()
    ]
    # Log weights that are not loaded with the pre-trained weights.
    if not_load_layers:
        for k in not_load_layers:
            logger.info("Network weights {} not loaded.".format(k))
    # Weights that do not have match from the pre-trained model.
    not_use_layers = [
        k for k in pre_train_dict.keys() if k not in pre_train_dict_match.keys()
    ]
    # Log weights that are not loaded with the pre-trained weights.
    if not_use_layers:
        for k in not_use_layers:
            logger.info("Network weights {} not used.".format(k))
    # Load pre-trained weights.
    ms.load_state_dict(pre_train_dict_match, strict=False)
    epoch = -1

    # Load the optimizer state (commonly not done when fine-tuning)
    if "epoch" in checkpoint.keys() and not epoch_reset:
        epoch = checkpoint["epoch"]
        if optimizer:
            optimizer.load_state_dict(checkpoint["optimizer_state"])
        if scaler:
            scaler.load_state_dict(checkpoint["scaler_state"])
    else:
        epoch = -1
    return epoch

class TransformerBasicHead(nn.Module):
    """
    Basic Transformer Head. No pool.
    """

    def __init__(
        self,
        dim_in,
        num_classes,
        dropout_rate=0.0,
        act_func="softmax",
    ):
        """
        Perform linear projection and activation as head for tranformers.
        Args:
            dim_in (int): the channel dimension of the input to the head.
            num_classes (int): the channel dimensions of the output to the head.
            dropout_rate (float): dropout rate. If equal to 0.0, perform no
                dropout.
            act_func (string): activation function to use. 'softmax': applies
                softmax on the output. 'sigmoid': applies sigmoid on the output.
        """
        super(TransformerBasicHead, self).__init__()
        if dropout_rate > 0.0:
            self.dropout = nn.Dropout(dropout_rate)
        self.projection = nn.Linear(dim_in, num_classes, bias=True)

        # Softmax for evaluation and testing.
        if act_func == "softmax":
            self.act = nn.Softmax(dim=1)
        elif act_func == "sigmoid":
            self.act = nn.Sigmoid()
        else:
            raise NotImplementedError(
                "{} is not supported as an activation" "function.".format(act_func)
            )

    def forward(self, x):
        if hasattr(self, "dropout"):
            x = self.dropout(x)
        x = self.projection(x)
        
        # LF - removing softmax activation function which makes model break in evaluation
        #if not self.training:
        #    x = self.act(x)
        return x

def _prepare_mvit_configs(cfg):
    """
    Prepare mvit configs for dim_mul and head_mul facotrs, and q and kv pooling
    kernels and strides.
    """
    depth = cfg.MVIT.DEPTH
    dim_mul, head_mul = torch.ones(depth + 1), torch.ones(depth + 1)
    for i in range(len(cfg.MVIT.DIM_MUL)):
        dim_mul[cfg.MVIT.DIM_MUL[i][0]] = cfg.MVIT.DIM_MUL[i][1]
    for i in range(len(cfg.MVIT.HEAD_MUL)):
        head_mul[cfg.MVIT.HEAD_MUL[i][0]] = cfg.MVIT.HEAD_MUL[i][1]

    pool_q = [[] for i in range(depth)]
    pool_kv = [[] for i in range(depth)]
    stride_q = [[] for i in range(depth)]
    stride_kv = [[] for i in range(depth)]

    for i in range(len(cfg.MVIT.POOL_Q_STRIDE)):
        stride_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_Q_STRIDE[i][1:]
        pool_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL

    # If POOL_KV_STRIDE_ADAPTIVE is not None, initialize POOL_KV_STRIDE.
    if cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE is not None:
        _stride_kv = cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE
        cfg.MVIT.POOL_KV_STRIDE = []
        for i in range(cfg.MVIT.DEPTH):
            if len(stride_q[i]) > 0:
                _stride_kv = [
                    max(_stride_kv[d] // stride_q[i][d], 1)
                    for d in range(len(_stride_kv))
                ]
            cfg.MVIT.POOL_KV_STRIDE.append([i] + _stride_kv)

    for i in range(len(cfg.MVIT.POOL_KV_STRIDE)):
        stride_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KV_STRIDE[i][1:]
        pool_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL

    return dim_mul, head_mul, pool_q, pool_kv, stride_q,  stride_kv


def _prepare_mvit_configs(cfg):
    """
    Prepare mvit configs for dim_mul and head_mul facotrs, and q and kv pooling
    kernels and strides.
    """
    depth = cfg.MVIT.DEPTH
    dim_mul, head_mul = torch.ones(depth + 1), torch.ones(depth + 1)
    for i in range(len(cfg.MVIT.DIM_MUL)):
        dim_mul[cfg.MVIT.DIM_MUL[i][0]] = cfg.MVIT.DIM_MUL[i][1]
    for i in range(len(cfg.MVIT.HEAD_MUL)):
        head_mul[cfg.MVIT.HEAD_MUL[i][0]] = cfg.MVIT.HEAD_MUL[i][1]

    pool_q = [[] for i in range(depth)]
    pool_kv = [[] for i in range(depth)]
    stride_q = [[] for i in range(depth)]
    stride_kv = [[] for i in range(depth)]

    for i in range(len(cfg.MVIT.POOL_Q_STRIDE)):
        stride_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_Q_STRIDE[i][1:]
        pool_q[cfg.MVIT.POOL_Q_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL

    # If POOL_KV_STRIDE_ADAPTIVE is not None, initialize POOL_KV_STRIDE.
    if cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE is not None:
        _stride_kv = cfg.MVIT.POOL_KV_STRIDE_ADAPTIVE
        cfg.MVIT.POOL_KV_STRIDE = []
        for i in range(cfg.MVIT.DEPTH):
            if len(stride_q[i]) > 0:
                _stride_kv = [
                    max(_stride_kv[d] // stride_q[i][d], 1)
                    for d in range(len(_stride_kv))
                ]
            cfg.MVIT.POOL_KV_STRIDE.append([i] + _stride_kv)

    for i in range(len(cfg.MVIT.POOL_KV_STRIDE)):
        stride_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KV_STRIDE[i][1:]
        pool_kv[cfg.MVIT.POOL_KV_STRIDE[i][0]] = cfg.MVIT.POOL_KVQ_KERNEL

    return dim_mul, head_mul, pool_q, pool_kv, stride_q,  stride_kv


class PatchEmbed(nn.Module):
    """
    PatchEmbed.
    """

    def __init__(
        self,
        dim_in=3,
        dim_out=768,
        kernel=(7, 7),
        stride=(4, 4),
        padding=(3, 3),
    ):
        super().__init__()

        self.proj = nn.Conv2d(
            dim_in,
            dim_out,
            kernel_size=kernel,
            stride=stride,
            padding=padding,
        )

    def forward(self, x):
        x = self.proj(x.to())
        # B C H W -> B HW C
        return x.flatten(2).transpose(1, 2), x.shape
class MViT(nn.Module):
    """
    Improved Multiscale Vision Transformers for Classification and Detection
    Yanghao Li*, Chao-Yuan Wu*, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik,
        Christoph Feichtenhofer*
    https://arxiv.org/abs/2112.01526
    Multiscale Vision Transformers
    Haoqi Fan*, Bo Xiong*, Karttikeya Mangalam*, Yanghao Li*, Zhicheng Yan, Jitendra Malik,
        Christoph Feichtenhofer*
    https://arxiv.org/abs/2104.11227
    """

    def __init__(self, cfg):
        super().__init__()
        # Get parameters.
        assert cfg.DATA.TRAIN_CROP_SIZE == cfg.DATA.TEST_CROP_SIZE
        # Prepare input.
        in_chans = 3
        spatial_size = cfg.DATA.TRAIN_CROP_SIZE
        # Prepare output.
        num_classes = cfg.MODEL.NUM_CLASSES
        embed_dim = cfg.MVIT.EMBED_DIM
        # MViT params.
        num_heads = cfg.MVIT.NUM_HEADS
        depth = cfg.MVIT.DEPTH
        self.cls_embed_on = cfg.MVIT.CLS_EMBED_ON
        self.use_abs_pos = cfg.MVIT.USE_ABS_POS
        self.zero_decay_pos_cls = cfg.MVIT.ZERO_DECAY_POS_CLS

        norm_layer = partial(nn.LayerNorm, eps=1e-6)

        if cfg.MODEL.ACT_CHECKPOINT:
            validate_checkpoint_wrapper_import(checkpoint_wrapper)

        patch_embed = PatchEmbed(
            dim_in=in_chans,
            dim_out=embed_dim,
            kernel=cfg.MVIT.PATCH_KERNEL,
            stride=cfg.MVIT.PATCH_STRIDE,
            padding=cfg.MVIT.PATCH_PADDING,
        )
        if cfg.MODEL.ACT_CHECKPOINT:
            patch_embed = checkpoint_wrapper(patch_embed)
        self.patch_embed = patch_embed

        patch_dims = [
            spatial_size // cfg.MVIT.PATCH_STRIDE[0],
            spatial_size // cfg.MVIT.PATCH_STRIDE[1],
        ]
        num_patches = math.prod(patch_dims)

        dpr = [
            x.item() for x in torch.linspace(0, cfg.MVIT.DROPPATH_RATE, depth)
        ]  # stochastic depth decay rule

        if self.cls_embed_on:
            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
            pos_embed_dim = num_patches + 1
        else:
            pos_embed_dim = num_patches

        if self.use_abs_pos:
            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_dim, embed_dim))

        # MViT backbone configs
        dim_mul, head_mul, pool_q, pool_kv, stride_q, stride_kv = _prepare_mvit_configs(
            cfg
        )

        input_size = patch_dims
        self.blocks = nn.ModuleList()
        for i in range(depth):
            num_heads = round_width(num_heads, head_mul[i])
            if cfg.MVIT.DIM_MUL_IN_ATT:
                dim_out = round_width(
                    embed_dim,
                    dim_mul[i],
                    divisor=round_width(num_heads, head_mul[i]),
                )
            else:
                dim_out = round_width(
                    embed_dim,
                    dim_mul[i + 1],
                    divisor=round_width(num_heads, head_mul[i + 1]),
                )
            attention_block = MultiScaleBlock(
                dim=embed_dim,
                dim_out=dim_out,
                num_heads=num_heads,
                input_size=input_size,
                mlp_ratio=cfg.MVIT.MLP_RATIO,
                qkv_bias=cfg.MVIT.QKV_BIAS,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                kernel_q=pool_q[i] if len(pool_q) > i else [],
                kernel_kv=pool_kv[i] if len(pool_kv) > i else [],
                stride_q=stride_q[i] if len(stride_q) > i else [],
                stride_kv=stride_kv[i] if len(stride_kv) > i else [],
                mode=cfg.MVIT.MODE,
                has_cls_embed=self.cls_embed_on,
                pool_first=cfg.MVIT.POOL_FIRST,
                rel_pos_spatial=cfg.MVIT.REL_POS_SPATIAL,
                rel_pos_zero_init=cfg.MVIT.REL_POS_ZERO_INIT,
                residual_pooling=cfg.MVIT.RESIDUAL_POOLING,
                dim_mul_in_att=cfg.MVIT.DIM_MUL_IN_ATT,
            )

            if cfg.MODEL.ACT_CHECKPOINT:
                attention_block = checkpoint_wrapper(attention_block)
            self.blocks.append(attention_block)

            if len(stride_q[i]) > 0:
                input_size = [
                    size // stride for size, stride in zip(input_size, stride_q[i])
                ]
            embed_dim = dim_out

        self.norm = norm_layer(embed_dim)

        self.head = TransformerBasicHead(
            embed_dim,
            num_classes,
            dropout_rate=cfg.MODEL.DROPOUT_RATE,
            act_func=cfg.MODEL.HEAD_ACT,
        )
        if self.use_abs_pos:
            trunc_normal_(self.pos_embed, std=0.02)
        if self.cls_embed_on:
            trunc_normal_(self.cls_token, std=0.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0.0)

        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0.0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        names = []
        if self.zero_decay_pos_cls:
            # add all potential params
            names = ["pos_embed", "rel_pos_h", "rel_pos_w", "cls_token"]

        return names

    def forward(self, x):
        x, bchw = self.patch_embed(x)

        H, W = bchw[-2], bchw[-1]
        B, N, C = x.shape

        if self.cls_embed_on:
            cls_tokens = self.cls_token.expand(B, -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)

        if self.use_abs_pos:
            x = x + self.pos_embed

        thw = [H, W]
        for blk in self.blocks:
            x, thw = blk(x, thw)

        x = self.norm(x)

        #if self.cls_embed_on:
        #    x = x[:, 0]
        #else:
        #    x = x.mean(1)
#
        x = self.head(x)
        return x

### <<<<<<<<<<<<<<<end of mvit model class

def predict(model_path, label_type,image_path ):
    # """ 
    # The predict module takes in the trained model, a label mapping file and an image data to predict Skin Lesion or Eye Damage conditions

    # The trained conditions are as below :
    # ...

    # Usage:
    #     python predict.py --model_path /data/model/skin_lesion_hykaml_model_v1.pyth --label_type skin  --image_path /data/images/skin_lesion_sample_img1.jpg

    #     python predict.py --model_path C:\Users\kenric\Documents\projects\usyd_cs10_2\checkpoints\MViTv2_L_in1k.pyth --label_map_path skin  --image_path C:\Users\kenric\Documents\projects\usyd_cs10_2\mvit\DATA\01_lesions\test\ACK\PAT_186_286_109.bmp
    # """
    # print(f"model_path, label_type, image_path: {model_path}, {label_type}, {image_path}")

    import collections
    curr_model = MViT(cfg)

    # load pretrained weight
    load_checkpoint(path_to_checkpoint=model_path, model=curr_model)
    curr_model.eval()


    # load label input
    label_mapping = {}
    if label_type == 'skin':
        label_mapping = {0:'Actinic Keratosis (ACK)', 
                            1:'Basal Cell Carcinnoma (BCC)',
                            2:'Benign Keratosis (BKL)',
                            3:'Melanoma (MEL)',
                            4:'Melanocytic Nevi (NEV)' }
    elif label_type == 'eye':
        label_mapping = {0:'Age related Macular_Degeneration', 
                            1:'Cataract',
                            2:'Diabetic Retinopathy',
                            3:'Glaucoma',
                            4:'Hypertensive Retinopathy',
                            5:'Normal Fundus',
                            6:'Pathological Myopia' }

    # load a folder of images and get predictions for each
    p = Path(image_path).glob('**/*')
    image_filepaths = [x.__str__() for x in p if x.is_file()]
    # start predicting all images
    all_best_preds = []
    if cfg.NUM_GPUS:
        # Transfer the data to the current GPU device.
        curr_model = curr_model.cuda()
    for path in image_filepaths:
        # load the image input into a tensor object
        input = Image.open(path)
        # print(f'Image - format, size, mode: {input.format}, {input.size}, {input.mode}')
        input = np.asarray(input)
        input = torch.tensor(input)
        input = input.permute(2,0,1)
        input = input[None,:,:,:].float()
        # print(f'Tensor - shape, dtype, device: {input.shape},{input.dtype},{input.device}')
       
        if cfg.NUM_GPUS:
            # Transfer the data to the current GPU device.
            input = input.cuda(non_blocking=True)
        # predict
        preds = curr_model(input)
        preds = torch.squeeze(torch.mean(preds,dim=1,keepdim=True),dim=1)
        # preds = preds.detach().numpy()

        best_pred = torch.argmax(preds,dim=1).detach().item()
        best_pred_label = label_mapping[best_pred]
        all_best_preds.append(best_pred_label)
    
    print(f'Predictions relating to {label_type} condition for all images in {image_path}:\n'  \
            + "\n".join(map("\t\t:\t\t".join,zip(image_filepaths,all_best_preds)))
        )
    print(f'Summary: {collections.Counter(all_best_preds)}')
    
    return all_best_preds

    

if __name__ == "__main__":
    print('You are using predict module. Please read the documentation on how-to-use guides.')
    parser = argparse.ArgumentParser(
                    prog = 'Predict Skin Lesion/ Eye Damage conditions from an image file ',
                    description = 'The program uses a pre-trained weight of a customised MViTv2 model to predict the medical conditions ',
                    epilog = 'Hope the program provides you with some useful insights. Please check with your medical professional for more conclusive diagnosis results')

    parser.add_argument('--model_path', help='path to your pretrained model weight')
    parser.add_argument('--image_path', help='path to your image as an input into the program')
    parser.add_argument('--label_type', choices = ['skin','eye'], help='skin or eye')

    args = parser.parse_args()

    print(f'Argument parser: {args}')

    predict(args.model_path,args.label_type,args.image_path)

   

